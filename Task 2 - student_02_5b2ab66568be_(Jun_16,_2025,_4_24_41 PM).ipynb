{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Y-JIXoyaOf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y-JIXoyaOf6",
        "outputId": "57a9e4fc-8c33-4f07-e61d-5af1e4ee72fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model 'all-MiniLM-L6-v2'...\n",
            "Generating embeddings for the FAQ data...\n",
            "Embeddings are ready.\n",
            "\n",
            "--- Chatbot is now active! ---\n",
            "Ask me a question. Type 'quit' or 'exit' to end the chat.\n",
            "------------------------------\n",
            "You: What are your shipping options?\n",
            "\n",
            "Bot: It looks like you're asking about 'What are your shipping options?'.\n",
            "Here is the information I found: We offer standard (5-7 business days), expedited (2-3 business days), and overnight shipping.\n",
            "\n",
            "You: quit\n",
            "\n",
            "Bot: Goodbye! Have a great day.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Configuration ---\n",
        "# IMPORTANT: Replace with your actual project and table details.\n",
        "PROJECT_ID = \"qwiklabs-gcp-02-e6e6123d96ed\"\n",
        "# The BigQuery table containing the questions, answers, and embeddings.\n",
        "BIGQUERY_TABLE = \"my_data_faq.aurora_bay_faq_embedded\" \n",
        "# The BigQuery embedding model used to create the vectors.\n",
        "BIGQUERY_EMBEDDING_MODEL = \"my_data_faq.Embeddings\"\n",
        "# Name of the environment variable for your API key.\n",
        "GEMINI_API_KEY_ENV_VAR = \"GEMINI_API_KEY\"\n",
        "# The Gemini model to use for generating responses.\n",
        "GEMINI_MODEL_NAME = \"gemini-pro\"\n",
        "\n",
        "\n",
        "# --- Helper function for printing (works in both terminal and notebooks) ---\n",
        "def is_notebook():\n",
        "    \"\"\"Checks if the code is running in a notebook environment.\"\"\"\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        if 'IPython' in get_ipython().__class__.__name__:\n",
        "            return True\n",
        "    except (ImportError, AttributeError):\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "# Use Markdown for rich output in notebooks, plain text for terminals.\n",
        "_IS_NOTEBOOK = is_notebook()\n",
        "if _IS_NOTEBOOK:\n",
        "    from IPython.display import display, Markdown\n",
        "\n",
        "def print_markdown(text):\n",
        "    \"\"\"Prints text as Markdown in notebooks, or plain text in terminals.\"\"\"\n",
        "    if _IS_NOTEBOOK:\n",
        "        display(Markdown(text))\n",
        "    else:\n",
        "        # A simple conversion for terminal display\n",
        "        text = text.replace('**', '') # Remove bold markers\n",
        "        text = text.replace('`', '')  # Remove code block markers\n",
        "        print(text)\n",
        "\n",
        "\n",
        "def initialize_services():\n",
        "    \"\"\"Initializes BigQuery and Gemini services.\"\"\"\n",
        "    print(\"--- Initializing Services ---\")\n",
        "    try:\n",
        "        bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "        print(f\"‚úÖ BigQuery client connected to project: {PROJECT_ID}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå BigQuery connection failed. Please check project ID and authentication. Error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        api_key = os.environ.get(GEMINI_API_KEY_ENV_VAR)\n",
        "        if not api_key:\n",
        "            raise ValueError(f\"API key not found. Please set the '{GEMINI_API_KEY_ENV_VAR}' environment variable.\")\n",
        "        \n",
        "        genai.configure(api_key=api_key)\n",
        "        model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
        "        print(f\"‚úÖ Gemini model '{GEMINI_MODEL_NAME}' is ready.\")\n",
        "        return bq_client, model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Gemini setup failed: {e}\")\n",
        "        return bq_client, None\n",
        "\n",
        "\n",
        "def search_knowledge_base(bq_client, user_question):\n",
        "    \"\"\"Searches the BigQuery knowledge base using VECTOR_SEARCH.\"\"\"\n",
        "    print(f\"üîç Searching for context related to: '{user_question}'\")\n",
        "    \n",
        "    # This query finds the single most relevant document using vector similarity.\n",
        "    search_query = f\"\"\"\n",
        "    SELECT\n",
        "        base.content,\n",
        "        base.question\n",
        "    FROM\n",
        "        VECTOR_SEARCH(\n",
        "            TABLE `{PROJECT_ID}.{BIGQUERY_TABLE}`,\n",
        "            'ml_generate_embedding_result',\n",
        "            (\n",
        "                SELECT ml_generate_embedding_result\n",
        "                FROM ML.GENERATE_EMBEDDING(\n",
        "                    MODEL `{PROJECT_ID}.{BIGQUERY_EMBEDDING_MODEL}`,\n",
        "                    (SELECT '{user_question}' AS content)\n",
        "                )\n",
        "            ),\n",
        "            top_k => 1,\n",
        "            -- This option improves performance on very large datasets.\n",
        "            options => '{{\"fraction_lists_to_search\": 0.01}}'\n",
        "        );\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        query_job = bq_client.query(search_query)\n",
        "        results = list(query_job.result()) # Use list() to wait for completion\n",
        "\n",
        "        if results:\n",
        "            # We return the 'content' (the answer) of the most similar row.\n",
        "            return results[0].content\n",
        "        return None # Return None if no results are found\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Vector search failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_response(model, user_question, context):\n",
        "    \"\"\"Generates a conversational AI response using the retrieved context.\"\"\"\n",
        "    print(\"ü§ñ Generating conversational response...\")\n",
        "    \n",
        "    # This prompt instructs the AI to be helpful but stay within the provided context.\n",
        "    system_prompt = f\"\"\"\n",
        "You are the Aurora Bay Information Assistant. Your role is to provide friendly and helpful answers based ONLY on the information provided below.\n",
        "If the answer is not in the information, you must politely state that you cannot find the answer in your knowledge base. Do not make up information.\n",
        "\n",
        "**Provided Information:**\n",
        "---\n",
        "{context}\n",
        "---\n",
        "\n",
        "**User's Question:** {user_question}\n",
        "\n",
        "**Your Answer:**\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(system_prompt)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Response generation failed: {e}\")\n",
        "        return \"I'm sorry, I encountered an issue and can't generate a response right now.\"\n",
        "\n",
        "\n",
        "def run_chatbot():\n",
        "    \"\"\"Main chatbot interaction loop.\"\"\"\n",
        "    bq_client, model = initialize_services()\n",
        "\n",
        "    if not bq_client or not model:\n",
        "        print(\"\\n‚ùå Chatbot cannot start due to initialization failure. Please check your configuration and credentials.\")\n",
        "        return\n",
        "    \n",
        "    print_markdown(\"\\n--- üèîÔ∏è **Aurora Bay Information Assistant** ---\")\n",
        "    print_markdown(\"üí¨ Ask me anything about our products, shipping, or returns!\")\n",
        "    print_markdown(\"*(Type 'quit' or 'exit' to end the conversation)*\\n\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"üë§ You: \").strip()\n",
        "\n",
        "            if user_input.lower() in {\"quit\", \"exit\", \"bye\"}:\n",
        "                print(\"\\nüëã Assistant: Thanks for visiting Aurora Bay! Have a great day! üåü\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            # Step 1: Search BigQuery for the most relevant information\n",
        "            context = search_knowledge_base(bq_client, user_input)\n",
        "\n",
        "            if not context:\n",
        "                print(\"\\nü§ñ Assistant: I'm sorry, I couldn't find any information related to that topic. Could you please try asking in a different way?\\n\")\n",
        "                continue\n",
        "\n",
        "            # Step 2: Use Gemini to generate a natural response from the found context\n",
        "            answer = generate_response(model, user_input, context)\n",
        "            \n",
        "            # Step 3: Display the final answer\n",
        "            print_markdown(f\"\\nü§ñ **Assistant:** {answer}\\n\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nüëã Assistant: Conversation ended. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è An unexpected error occurred: {e}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_chatbot()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "student-02-5b2ab66568be (Jun 16, 2025, 4:24:41‚ÄØPM)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
